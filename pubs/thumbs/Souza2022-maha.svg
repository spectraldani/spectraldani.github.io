<svg xmlns="http://www.w3.org/2000/svg" width="612" height="792" preserveAspectRatio="none"><path stroke="#000" stroke-width="4" d="M108 81h396"/><text x="151" y="113" font-size="17.2">Deep Mahalanobis Gaussian Process
</text><path stroke="#000" d="M108 132h396"/><g font-size="9"><text x="165" y="169">Daniel Augusto de Souza</text><text x="161" y="180">University College London</text><text x="154" y="191" font-family="monospace">daniel.souza.21@ucl.ac.uk</text></g><g font-size="9"><text x="359" y="169">Diego Mesquita
</text><text x="334" y="180">Getulio Vargas Foundation</text><text x="338" y="191" font-family="monospace">diego.mesquita@fgv.br</text></g><g font-size="9"><text x="164" y="219" transform="translate(9)">César Lincoln Mattos</text><text x="149" y="230" transform="translate(9)">Federal University of Ceará</text><text x="153" y="241" font-family="monospace" transform="translate(9)">cesarlincoln@dc.ufc.br</text></g><g font-size="9"><text x="348" y="219" transform="translate(7)">João Paulo Gomes</text><text x="324" y="230" transform="translate(7)">Federal University of Ceará</text><text x="344" y="241" font-family="monospace" transform="translate(7)">jpaulo@dc.ufc.br</text></g><text x="280" y="282" font-size="12">Abstract
</text><text x="108 127 132" y="449" font-size="12">1 Introduction
</text><text font-size="9"><tspan x="108" y="730">2022 NeurIPS Workshop on Gaussian Processes, Spatiotemporal Modeling, and Decision-making</tspan><tspan x="108" y="740">Systems.</tspan>
</text><text font-size="9" font-weight="400" style="line-height:1.07;text-align:justify" transform="translate(73 110)"><tspan x="34" y="360"><tspan dx="0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1">Gaussian processes (GP) priors are a non-parametric alternative to more traditional learn- </tspan></tspan><tspan x="34" y="370"><tspan dx="0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0 0 2 0 0 0 0 0 2">ing methods in various tasks in machine learning, however, the flexibility of this prior is </tspan></tspan><tspan x="34" y="380"><tspan dx="0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1">mostly determined by the kernel function used, therefore, expressive kernels with tuneable </tspan></tspan><tspan x="34" y="390"><tspan dx="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0">hyperparameters are the most common choices. Deep Gaussian processes (DGP) priors [5, 1] </tspan></tspan><tspan x="34" y="400"><tspan dx="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0">move away from expert-designed kernels and instead learn feature spaces from data without </tspan></tspan><tspan x="34" y="410"><tspan dx="0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1">any feature engineering, this is done through the outputs-to-inputs composition of functions </tspan></tspan><tspan x="34" y="419"><tspan dx="0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0 2 0 0 0 0 2 0 0 0 0 0 2">sampled from simple GP priors. However, naïve composition of GPs adds extra complica- </tspan></tspan><tspan x="34" y="429"><tspan dx="0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1">tions not present in, e.g., stacked neural network layers. As discussed in the literature, this </tspan></tspan><tspan x="34" y="439">stems from the non-injective transformations learned by each GP unit [7, 3].</tspan><tspan x="34" y="459"><tspan dx="0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1">Nevertheless, as described by Dunlop et al. [2], this input-output compositional setup is not </tspan></tspan><tspan x="34" y="469"><tspan dx="0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2">the only way to compose GPs. After all, the parameters of the GP distribution (mean and </tspan></tspan><tspan x="34" y="479"><tspan dx="0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 4 0 0 0 4 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 4 0 0 4 0 0 0 0 0 0 0 0 0 0 0 4 0 0 4 0 0 0 0 0 4 0 0 4 0 0 0 4 0 0 0 0 0 0 0 0 0 4">kernel function) are function themselves. In particular, we focus on the tradition of </tspan></tspan><tspan x="34" y="488"><tspan dx="0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1">considering the lengthscales of stationary kernels </tspan><tspan dx="1">ℓ</tspan><tspan dx="0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1"> to be functions of the inputs </tspan><tspan dx="1">ℓ</tspan><tspan dx="0 0 0 0 1 0 0 1">(x) as a </tspan></tspan><tspan x="34" y="498">way to make deep non-stationary GPs.</tspan><tspan x="34" y="518"><tspan dx="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0">The model we propose follows the footsteps of Titsias and Lázaro-Gredilla [9] which presents </tspan></tspan><tspan x="34" y="528"><tspan dx="0 0 6 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 6 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 6 0 0 6">a variational inference algorithm for marginalizing the lengthscale matrix of the </tspan></tspan><tspan x="34" y="538"><tspan dx="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0">Mahalanobis distance kernel, a generalization of the RBF kernel. By extending their prior to </tspan></tspan><tspan x="34" y="547"><tspan dx="0 0 4 0 0 4 0 0 0 0 0 0 4 0 0 4 0 0 0 0 0 0 4 0 4 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 4 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 4 0 0 4">a GP prior, we obtain a hierarchical GP model which is not susceptible to classical </tspan></tspan><tspan x="34" y="557"><tspan dx="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0">pathologies of DGPs, as shown in Figure 1, can learn non-stationary behaviour and has a bias </tspan></tspan><tspan x="34" y="567"><tspan dx="0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1">for dimensionality reduction. We approximate the posterior using variational inference and </tspan></tspan><tspan x="34" y="577"><tspan dx="0 0 0 0 0 4 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 4 0 0 4 0 0 0 0 0 0 0 4 0 0 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 4 0 0 0 0 4">call deep variational Mahalanobis GP (DVMGP) the model built on this approximate </tspan></tspan><tspan x="34" y="587">posterior.</tspan>
</text><text font-size="9" font-weight="400" style="line-height:1.2;text-align:justify" transform="translate(0 -6)"><tspan x="142" y="305"><tspan dx="0 0 0 2 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0 2">We propose a class of hierarchical Gaussian process priors in which each</tspan></tspan><tspan x="142" y="316"><tspan dx="0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 3 0 0 0 3 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 3 0 0 0 3 0 0 0 0 0 3 0 0 0 0 0 3 0 0 0 0 3 0 0 0 3">layer controls the kernel lengthscales of the next. While this has been</tspan></tspan><tspan x="142" y="327"><tspan dx="0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 3 0 0 0 3 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 3 0 0 0 0 3 0 0 3 0 0 0 3">explored before, our proposal extends previous work on the Mahalanobis</tspan></tspan><tspan x="142" y="338"><tspan dx="0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2">distance kernel bringing an alternative construction of non-stationary RBF-</tspan></tspan><tspan x="142" y="349"><tspan dx="0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 2">style kernels. The new approach has desirable properties that enables the</tspan></tspan><tspan x="142" y="359"><tspan dx="0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1">analysis of input-dependent lengthscales. More specifically, we interpret our</tspan></tspan><tspan x="142" y="370"><tspan dx="0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1">model as a GP that performs locally non-linear dimensionality reduction. We</tspan></tspan><tspan x="142" y="381"><tspan dx="0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 3 0 0 3 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 3 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 3 0 3">directly compare it with compositional deep Gaussian process, a popular</tspan></tspan><tspan x="142" y="392"><tspan dx="0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1">model that uses successive latent space mappings to alleviate the burden of</tspan></tspan><tspan x="142" y="403"><tspan dx="0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1">choosing a kernel function. Our experiments show promising results in both</tspan></tspan><tspan x="142" y="413">synthetic and real regression datasets.</tspan>
</text></svg>