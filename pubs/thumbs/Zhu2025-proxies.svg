<svg xmlns="http://www.w3.org/2000/svg" font-family="serif" version="1.2" viewBox="0 0 816 1056"><style>@supports (-moz-appearance:none){text{letter-spacing:-.18ex} #title{letter-spacing:-.4ex}}text{font-size:13px}</style><path fill="none" stroke="#000" stroke-width="2" d="M69 83h648" transform-origin="7141.8px -484.36px"/><path fill="none" stroke="#000" stroke-width="2" d="M69 147h648" style="transform-box:fill-box" transform-origin="50% 50%"/><path fill="none" stroke="#000" d="M69 833h77m259 79h77"/><text id="title" x="-2201" y="-3437" transform-origin="398.243px 529.482px"><tspan font-size="19.1" font-weight="700"><tspan x="82" y="121"><tspan>When Can Proxies Improve the Sample Complexity of Preference Learning?</tspan></tspan></tspan></text><path fill="none" d="M115 178h71v14h-71v-14Z" transform-origin="398.243px 529.482px"/><text x="-3056" y="-5125" transform-origin="398.243px 529.482px"><tspan font-weight="700"><tspan x="115" y="187"><tspan>Yuchen Zhu</tspan></tspan></tspan></text><text x="-4876" y="-5006" transform-origin="398.243px 529.482px"><tspan font-size="9.3"><tspan x="187" y="182"><tspan>1</tspan></tspan></tspan></text><text x="-5219" y="-5125" transform-origin="398.243px 529.482px"><tspan font-weight="700"><tspan x="200" y="187"><tspan>Daniel Augusto de Souza</tspan></tspan></tspan></text><text x="-8857" y="-5006" transform-origin="398.243px 529.482px"><tspan font-size="9.3"><tspan x="343" y="182"><tspan>1</tspan></tspan></tspan></text><text x="-9201" y="-5125" transform-origin="398.243px 529.482px"><tspan font-weight="700"><tspan x="356" y="187"><tspan>Zhengyan Shi</tspan></tspan></tspan></text><text x="-11259" y="-5006" transform-origin="398.243px 529.482px"><tspan font-size="9.3"><tspan x="437" y="182"><tspan>1</tspan></tspan></tspan></text><text x="-11602" y="-5125" transform-origin="398.243px 529.482px"><tspan font-weight="700"><tspan x="451" y="187"><tspan>Mengyue Yang</tspan></tspan></tspan></text><text x="-13819" y="-5006" transform-origin="398.243px 529.482px"><tspan font-size="9.3"><tspan x="538" y="182"><tspan>2</tspan></tspan></tspan></text><text x="-14163" y="-5125" transform-origin="398.243px 529.482px"><tspan font-weight="700"><tspan x="551" y="187"><tspan>Pasquale Minervini</tspan></tspan></tspan></text><text x="-17040" y="-5006" transform-origin="398.243px 529.482px"><tspan font-size="9.3"><tspan x="664" y="182"><tspan>3</tspan></tspan></tspan></text><text x="-7236" y="-5530" transform-origin="398.243px 529.482px"><tspan font-weight="700"><tspan x="279" y="203"><tspan>Alexander Dâ€™Amour</tspan></tspan></tspan></text><text x="-10270" y="-5412" transform-origin="398.243px 529.482px"><tspan font-size="9.3"><tspan x="398" y="198"><tspan>4</tspan></tspan></tspan></text><text x="-10613" y="-5530" transform-origin="398.243px 529.482px"><tspan font-weight="700"><tspan x="412" y="203"><tspan>Matt J. Kusner</tspan></tspan></tspan></text><text x="-12866" y="-5412" transform-origin="398.243px 529.482px"><tspan font-size="9.3"><tspan x="500" y="198"><tspan>1</tspan></tspan></tspan></text><text x="-5104" y="-6670" transform-origin="398.243px 529.482px"><tspan font-size="15.9" font-weight="700"><tspan x="196" y="248"><tspan>Abstract</tspan></tspan></tspan></text><text x="-2563" y="-7229" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="270"><tspan>We address the problem of</tspan><tspan font-style="italic"> reward hacking</tspan><tspan>, where</tspan></tspan></tspan></text><text x="-2563" y="-7635" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="286"><tspan>maximising a proxy reward does not necessarily</tspan></tspan></tspan></text><text x="-2563" y="-8041" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="302"><tspan>increase the true reward.</tspan></tspan></tspan></text><text x="-6159" y="-8041" transform-origin="398.243px 529.482px"><tspan><tspan x="237" y="302"><tspan>This is a key concern</tspan></tspan></tspan></text><text x="-2563" y="-8447" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="318"><tspan>for Large Language Models (LLMs), as they are</tspan></tspan></tspan></text><text x="-2563" y="-8854" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="334"><tspan>often fine-tuned on human preferences that may</tspan></tspan></tspan></text><text x="-2563" y="-9259" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="350"><tspan>not accurately reflect a true objective.</tspan></tspan></tspan></text><text x="-8004" y="-9259" transform-origin="398.243px 529.482px"><tspan><tspan x="309" y="350"><tspan>Existing</tspan></tspan></tspan></text><text x="-2551" y="-9665" transform-origin="398.243px 529.482px"><tspan><tspan x="95" y="365"><tspan>work uses various tricks such as regularisation,</tspan></tspan></tspan></text><text x="-2563" y="-10071" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="381"><tspan>tweaks to the reward model, and reward hacking</tspan></tspan></tspan></text><text x="-2563" y="-10476" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="397"><tspan>detectors, to limit the influence that such proxy</tspan></tspan></tspan></text><text x="-2563" y="-10882" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="413"><tspan>preferences have on a model.</tspan></tspan></tspan></text><text x="-6793" y="-10882" transform-origin="398.243px 529.482px"><tspan><tspan x="262" y="413"><tspan>Luckily, in many</tspan></tspan></tspan></text><text x="-2563" y="-11288" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="429"><tspan>contexts such as medicine, education, and law, a</tspan></tspan></tspan></text><text x="-2563" y="-11694" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="445"><tspan>sparse amount of expert data is often available. In</tspan></tspan></tspan></text><text x="-2563" y="-12100" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="461"><tspan>these cases, it is often unclear whether the addi-</tspan></tspan></tspan></text><text x="-2563" y="-12505" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="477"><tspan>tion of proxy data can improve policy learning.</tspan></tspan></tspan></text><text x="-2547" y="-12911" transform-origin="398.243px 529.482px"><tspan><tspan x="95" y="493"><tspan>We outline a set of sufficient conditions on proxy</tspan></tspan></tspan></text><text x="-2563" y="-13317" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="509"><tspan>feedback that, if satisfied, indicate that proxy data</tspan></tspan></tspan></text><text x="-2563" y="-13723" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="525"><tspan>can provably improve the sample complexity of</tspan></tspan></tspan></text><text x="-2563" y="-14128" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="541"><tspan>learning the ground truth policy.</tspan></tspan></tspan></text><text x="-7381" y="-14128" transform-origin="398.243px 529.482px"><tspan><tspan x="285" y="541"><tspan>These condi-</tspan></tspan></tspan></text><text x="-2563" y="-14534" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="557"><tspan>tions can inform the data collection process for</tspan></tspan></tspan></text><text x="-2563" y="-14940" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="573"><tspan>specific tasks. The result implies a parameterisa-</tspan></tspan></tspan></text><text x="-2563" y="-15346" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="589"><tspan>tion for LLMs that achieves this improved sample</tspan></tspan></tspan></text><text x="-2563" y="-15752" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="605"><tspan>complexity.</tspan></tspan></tspan></text><text x="-4324" y="-15752" transform-origin="398.243px 529.482px"><tspan><tspan x="165" y="605"><tspan>We detail how one can adapt exist-</tspan></tspan></tspan></text><text x="-2563" y="-16158" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="620"><tspan>ing architectures to yield this improved sample</tspan></tspan></tspan></text><text x="-2563" y="-16564" transform-origin="398.243px 529.482px"><tspan><tspan x="96" y="636"><tspan>complexity.</tspan></tspan></tspan></text><text x="-1887" y="-17775" transform-origin="398.243px 529.482px"><tspan font-size="15.9" font-weight="700"><tspan x="69" y="684"><tspan>1. Introduction</tspan></tspan></tspan></text><text x="-1887" y="-18434" transform-origin="398.243px 529.482px"><tspan><tspan x="69" y="710"><tspan>Large Language Models (LLMs) have revolutionised ma-</tspan></tspan></tspan></text><text x="-1887" y="-18839" transform-origin="398.243px 529.482px"><tspan><tspan x="69" y="726"><tspan>chine learning with their surprising capabilities, surpassing</tspan></tspan></tspan></text><text x="-1887" y="-19245" transform-origin="398.243px 529.482px"><tspan><tspan x="69" y="742"><tspan>human-level performance in law, medicine, and other exam-</tspan></tspan></tspan></text><text x="-1887" y="-19651" transform-origin="398.243px 529.482px"><tspan><tspan x="69" y="758"><tspan>inations (</tspan><tspan fill="#001473">Achiam et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2023</tspan><tspan>;</tspan><tspan fill="#001473"> Amin et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2023</tspan><tspan>).</tspan></tspan></tspan></text><text x="-8809" y="-19651" transform-origin="398.243px 529.482px"><tspan><tspan x="341" y="758"><tspan>A large</tspan></tspan></tspan></text><text x="-1887" y="-20057" transform-origin="398.243px 529.482px"><tspan><tspan x="69" y="774"><tspan>part of their success is their ability to incorporate human</tspan></tspan></tspan></text><text x="-1887" y="-20462" transform-origin="398.243px 529.482px"><tspan><tspan x="69" y="790"><tspan>preferences to learn complex objectives such as trustworthi-</tspan></tspan></tspan></text><text x="-1887" y="-20869" transform-origin="398.243px 529.482px"><tspan><tspan x="69" y="806"><tspan>ness (</tspan><tspan fill="#001473">Yu et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2024</tspan><tspan>), sentiment preferences (</tspan><tspan fill="#001473">Chakraborty</tspan></tspan></tspan></text><text x="-1887" y="-21275" transform-origin="398.243px 529.482px"><tspan><tspan x="69" y="821"><tspan fill="#001473">et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2024</tspan><tspan>), and value alignment (</tspan><tspan fill="#001473">Ji et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2023</tspan><tspan>).</tspan></tspan></tspan></text><text x="-2209" y="-21789" transform-origin="398.243px 529.482px"><tspan font-size="8"><tspan x="82" y="842"><tspan>*</tspan></tspan></tspan></text><text x="-2328" y="-21918" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="87" y="847"><tspan>Equal contribution</tspan></tspan></tspan></text><text x="-4828" y="-21789" transform-origin="398.243px 529.482px"><tspan font-size="8"><tspan x="185" y="842"><tspan>1</tspan></tspan></tspan></text><text x="-4947" y="-21918" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="189" y="847"><tspan>Department of Computer Science, Uni-</tspan></tspan></tspan></text><text x="-1879" y="-22257" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="69" y="860"><tspan>versity College London</tspan></tspan></tspan></text><text x="-4867" y="-22128" transform-origin="398.243px 529.482px"><tspan font-size="8"><tspan x="186" y="855"><tspan>2</tspan></tspan></tspan></text><text x="-4985" y="-22257" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="191" y="860"><tspan>University of Bristol</tspan></tspan></tspan></text><text x="-7610" y="-22128" transform-origin="398.243px 529.482px"><tspan font-size="8"><tspan x="294" y="855"><tspan>3</tspan></tspan></tspan></text><text x="-7728" y="-22257" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="299" y="860"><tspan>University of Ed-</tspan></tspan></tspan></text><text x="-1887" y="-22595" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="69" y="873"><tspan>inburgh</tspan></tspan></tspan></text><text x="-2952" y="-22466" transform-origin="398.243px 529.482px"><tspan font-size="8"><tspan x="111" y="868"><tspan>4</tspan></tspan></tspan></text><text x="-3070" y="-22595" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="116" y="873"><tspan>Google Deepmind.</tspan></tspan></tspan></text><text x="-5666" y="-22595" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="218" y="873"><tspan>Correspondence to:</tspan></tspan></tspan></text><text x="-8287" y="-22595" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="321" y="873"><tspan>Yuchen Zhu</tspan></tspan></tspan></text><text x="-1887" y="-22935" transform-origin="398.243px 529.482px"><tspan font-size="11.9" font-style="italic"><tspan x="69" y="887"><tspan>&lt;</tspan></tspan></tspan></text><text x="-2130" y="-22933" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="79" y="887"><tspan>yuchen.zhu.18@ucl.ac.uk</tspan></tspan></tspan></text><text x="-5284" y="-22935" transform-origin="398.243px 529.482px"><tspan font-size="11.9" font-style="italic"><tspan x="203" y="887"><tspan>&gt;</tspan></tspan></tspan></text><text x="-5527" y="-22933" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="212" y="887"><tspan>.</tspan></tspan></tspan></text><text x="-1877" y="-23610" transform-origin="398.243px 529.482px"><tspan font-size="11.9" font-style="italic"><tspan x="69" y="913"><tspan>Proceedings of the</tspan></tspan></tspan></text><text x="-4327" y="-23612" transform-origin="398.243px 529.482px"><tspan font-size="11.9" font-style="italic"><tspan x="165" y="913"><tspan>42</tspan></tspan></tspan></text><text x="-4688" y="-23480" transform-origin="398.243px 529.482px"><tspan font-size="8" font-style="italic"><tspan x="179" y="908"><tspan>nd</tspan></tspan></tspan></text><text x="-5088" y="-23610" transform-origin="398.243px 529.482px"><tspan font-size="11.9" font-style="italic"><tspan x="195" y="913"><tspan>International Conference on Machine</tspan></tspan></tspan></text><text x="-1878" y="-23948" transform-origin="398.243px 529.482px"><tspan font-size="11.9" font-style="italic"><tspan x="69" y="926"><tspan>Learning</tspan><tspan font-style="normal">, Vancouver, Canada. PMLR 267, 2025. Copyright 2025</tspan></tspan></tspan></text><text x="-1887" y="-24286" transform-origin="398.243px 529.482px"><tspan font-size="11.9"><tspan x="69" y="940"><tspan>by the author(s).</tspan></tspan></tspan></text><text x="-10441" y="-6671" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="248"><tspan>In many cases, this preference data is a</tspan><tspan font-style="italic"> proxy</tspan><tspan> for the ground</tspan></tspan></tspan></text><text x="-10441" y="-7077" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="264"><tspan>truth.</tspan></tspan></tspan></text><text x="-11395" y="-7077" transform-origin="398.243px 529.482px"><tspan><tspan x="443" y="264"><tspan>For example,</tspan></tspan></tspan></text><text x="-13373" y="-7077" transform-origin="398.243px 529.482px"><tspan><tspan x="520" y="264"><tspan>humans raters tend to prefer longer</tspan></tspan></tspan></text><text x="-10441" y="-7482" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="280"><tspan>answers to a question, even if the answer is less informative</tspan></tspan></tspan></text><text x="-10429" y="-7888" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="296"><tspan>(</tspan><tspan fill="#001473">Zhou et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2024</tspan><tspan>). In this case, â€˜response lengthâ€™ is a</tspan><tspan font-style="italic"> proxy</tspan></tspan></tspan></text><text x="-10441" y="-8294" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="312"><tspan>for the true helpfulness of an answer. If an LLM is trained</tspan></tspan></tspan></text><text x="-10441" y="-8701" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="328"><tspan>on this proxy data alone it leads to a â€˜length-biasâ€™ (</tspan><tspan fill="#001473">Shen</tspan></tspan></tspan></text><text x="-10441" y="-9106" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="344"><tspan fill="#001473">et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2023</tspan><tspan>;</tspan><tspan fill="#001473"> Singhal et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2023</tspan><tspan>), as LLMs fine-tuned with</tspan></tspan></tspan></text><text x="-10441" y="-9512" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="359"><tspan>this preference data generate longer and better formatted</tspan></tspan></tspan></text><text x="-10441" y="-9918" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="375"><tspan>responses to appear more helpful (</tspan><tspan fill="#001473">Chen et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2024</tspan><tspan>). This</tspan></tspan></tspan></text><text x="-10441" y="-10324" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="391"><tspan>is an example of the well-known phenomenon of</tspan><tspan font-style="italic"> reward</tspan></tspan></tspan></text><text x="-10441" y="-10729" transform-origin="398.243px 529.482px"><tspan font-style="italic"><tspan x="405" y="407"><tspan>hacking</tspan></tspan></tspan></text><text x="-11483" y="-10607" transform-origin="398.243px 529.482px"><tspan font-size="9.3"><tspan x="446" y="402"><tspan fill="#001473">1</tspan></tspan></tspan></text><text x="-11619" y="-10729" transform-origin="398.243px 529.482px"><tspan><tspan x="451" y="407"><tspan>: a model optimised to perform well with respect to</tspan></tspan></tspan></text><text x="-10441" y="-11135" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="423"><tspan>a proxy reward function, performs poorly with respect to a</tspan></tspan></tspan></text><text x="-10441" y="-11541" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="439"><tspan>ground truth reward function (</tspan><tspan fill="#001473">Casper et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2023</tspan><tspan>). Reward</tspan></tspan></tspan></text><text x="-10441" y="-11947" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="455"><tspan>hacking is a fundamental problem in learning that has been</tspan></tspan></tspan></text><text x="-10441" y="-12352" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="471"><tspan>observed in optimised circuits listening in on the oscilla-</tspan></tspan></tspan></text><text x="-10441" y="-12758" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="487"><tspan>tors of other computers when instead tasked to build their</tspan></tspan></tspan></text><text x="-10441" y="-13164" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="503"><tspan>own (</tspan><tspan fill="#001473">Bird &amp; Layzell</tspan><tspan>,</tspan><tspan fill="#001473"> 2002</tspan><tspan>), universities rejecting the most</tspan></tspan></tspan></text><text x="-10441" y="-13570" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="519"><tspan>qualified applicants to boost their ratings (</tspan><tspan fill="#001473">Golden</tspan><tspan>,</tspan><tspan fill="#001473"> 2001</tspan><tspan>),</tspan></tspan></tspan></text><text x="-10441" y="-13975" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="535"><tspan>and many other cases in game playing (</tspan><tspan fill="#001473">Clark &amp; Amodei</tspan><tspan>,</tspan></tspan></tspan></text><text x="-10441" y="-14381" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="551"><tspan fill="#001473">2016</tspan><tspan>),</tspan></tspan></tspan></text><text x="-11453" y="-14381" transform-origin="398.243px 529.482px"><tspan><tspan x="445" y="551"><tspan>autonomous driving (</tspan><tspan fill="#001473">Knox et al.</tspan><tspan>,</tspan></tspan></tspan></text><text x="-16248" y="-14381" transform-origin="398.243px 529.482px"><tspan><tspan x="633" y="551"><tspan fill="#001473">2023</tspan><tspan>),</tspan></tspan></tspan></text><text x="-17259" y="-14381" transform-origin="398.243px 529.482px"><tspan><tspan x="673" y="551"><tspan>and text</tspan></tspan></tspan></text><text x="-10441" y="-14787" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="567"><tspan>summarisation (</tspan><tspan fill="#001473">Paulus et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2018</tspan><tspan>).</tspan></tspan></tspan></text><text x="-10430" y="-15396" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="591"><tspan>To address reward hacking in LLMs, prior work largely de-</tspan></tspan></tspan></text><text x="-10441" y="-15802" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="607"><tspan>signs tweaks to the model, data, and optimization procedure.</tspan></tspan></tspan></text><text x="-10430" y="-16208" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="622"><tspan>This includes regularisation towards an initial policy (</tspan><tspan fill="#001473">Schul-</tspan></tspan></tspan></text><text x="-10441" y="-16614" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="638"><tspan fill="#001473">man et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2017</tspan><tspan>;</tspan><tspan fill="#001473"> Rafailov et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2023</tspan><tspan>;</tspan><tspan fill="#001473"> Huang et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2024</tspan><tspan>),</tspan></tspan></tspan></text><text x="-10441" y="-17020" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="654"><tspan>changing properties of the reward model (</tspan><tspan fill="#001473">Gao et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2023</tspan><tspan>;</tspan></tspan></tspan></text><text x="-10441" y="-17425" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="670"><tspan fill="#001473">Coste et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2024</tspan><tspan>), using soft labels (</tspan><tspan fill="#001473">Zhu et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2024</tspan><tspan>), ad-</tspan></tspan></tspan></text><text x="-10441" y="-17831" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="686"><tspan>justing optimization hyperparameters (</tspan><tspan fill="#001473">Singhal et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2023</tspan><tspan>),</tspan></tspan></tspan></text><text x="-10441" y="-18237" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="702"><tspan>reward hacking detection mechanisms (</tspan><tspan fill="#001473">Pan et</tspan></tspan></tspan></text><text x="-17079" y="-18237" transform-origin="398.243px 529.482px"><tspan><tspan x="666" y="702"><tspan fill="#001473">al.</tspan><tspan>,</tspan></tspan></tspan></text><text x="-17626" y="-18237" transform-origin="398.243px 529.482px"><tspan><tspan x="687" y="702"><tspan fill="#001473">2022</tspan><tspan>;</tspan></tspan></tspan></text><text x="-10441" y="-18643" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="718"><tspan fill="#001473">Miao et al.</tspan><tspan>,</tspan></tspan></tspan></text><text x="-12209" y="-18643" transform-origin="398.243px 529.482px"><tspan><tspan x="475" y="718"><tspan fill="#001473">2024</tspan><tspan>),</tspan></tspan></tspan></text><text x="-13233" y="-18643" transform-origin="398.243px 529.482px"><tspan><tspan x="515" y="718"><tspan>and introducing additional tools spe-</tspan></tspan></tspan></text><text x="-10441" y="-19048" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="734"><tspan>cialised to counteract length bias (</tspan><tspan fill="#001473">Chen et al.</tspan><tspan>,</tspan><tspan fill="#001473"> 2024</tspan><tspan>).</tspan></tspan></tspan></text><text x="-17847" y="-19048" transform-origin="398.243px 529.482px"><tspan><tspan x="696" y="734"><tspan>The</tspan></tspan></tspan></text><text x="-10441" y="-19454" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="750"><tspan>reasoning behind this comes from the makeup of proxy data.</tspan></tspan></tspan></text><text x="-10425" y="-19860" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="766"><tspan>We can think of proxy data as having two parts: (i) a</tspan><tspan font-style="italic"> true</tspan></tspan></tspan></text><text x="-10441" y="-20266" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="782"><tspan>part that brings a policy closer to the ground truth policy</tspan></tspan></tspan></text><text x="-10441" y="-20671" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="798"><tspan>during learning and (ii) a</tspan><tspan font-style="italic"> false</tspan><tspan> part that moves it farther</tspan></tspan></tspan></text><text x="-10441" y="-21077" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="814"><tspan>away. Prior work limits learning to reduce the impact that</tspan></tspan></tspan></text><text x="-10441" y="-21483" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="830"><tspan>the false part has on the final model.</tspan></tspan></tspan></text><text x="-10425" y="-22092" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="854"><tspan>Without any further information on proxy preferences or</tspan></tspan></tspan></text><text x="-10441" y="-22498" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="870"><tspan>the ground truth,</tspan></tspan></tspan></text><text x="-13035" y="-22498" transform-origin="398.243px 529.482px"><tspan><tspan x="507" y="870"><tspan>we are restricted to methods such as</tspan></tspan></tspan></text><text x="-10441" y="-22904" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="885"><tspan>these,</tspan></tspan></tspan></text><text x="-11393" y="-22904" transform-origin="398.243px 529.482px"><tspan><tspan x="443" y="885"><tspan>i.e.,</tspan></tspan></tspan></text><text x="-12056" y="-22904" transform-origin="398.243px 529.482px"><tspan><tspan x="469" y="885"><tspan>methods that</tspan></tspan></tspan></text><text x="-14028" y="-22904" transform-origin="398.243px 529.482px"><tspan><tspan x="546" y="885"><tspan>are blind to the true and false</tspan></tspan></tspan></text><text x="-10441" y="-23310" transform-origin="398.243px 529.482px"><tspan><tspan x="405" y="901"><tspan>parts of proxy data, to reduce the impact of reward hack-</tspan></tspan></tspan></text><text x="-10870" y="-23824" transform-origin="398.243px 529.482px"><tspan font-size="8"><tspan x="422" y="922"><tspan>1</tspan></tspan></tspan></text><text stroke-width="2" font-size="11.9" transform-origin="398.243px 529.482px"><tspan x="426" y="927">This is also sometimes referred to as</tspan><tspan font-style="italic"> reward over-optimisation</tspan><tspan>.</tspan></text><text x="-10050" y="-25132" transform-origin="398.243px 529.482px"><tspan><tspan x="390" y="973"><tspan>1</tspan></tspan></tspan></text></svg>